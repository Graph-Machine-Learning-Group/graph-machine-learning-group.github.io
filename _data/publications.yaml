---
- title: 'Learning Latent Graph Structures and their Uncertainty'
  links:
    paper: https://arxiv.org/abs/2405.19933
  venue: preprint
  year: 2024
  authors:
  - id:amanenti
  - id:dzambon
  - id:calippi
  keywords:
    - graph structure learning
    - graph neural networks
    - model calibration
  abstract: Within a prediction task, Graph Neural Networks (GNNs) use relational information as an inductive bias to enhance the model's accuracy. As task-relevant relations might be unknown, graph structure learning approaches have been proposed to learn them while solving the downstream prediction task. In this paper, we demonstrate that minimization of a point-prediction loss function, e.g., the mean absolute error, does not guarantee proper learning of the latent relational information and its associated uncertainty. Conversely, we prove that a suitable loss function on the stochastic model outputs simultaneously grants (i) the unknown adjacency matrix latent distribution and (ii) optimal performance on the prediction task. Finally, we propose a sampling-based method that solves this joint learning task. Empirical results validate our theoretical claims and demonstrate the effectiveness of the proposed approach.
  bibtex: >
    @misc{manenti2024learning,
      title = {Learning {{Latent Graph Structures}} and Their {{Uncertainty}}},
      author = {Manenti, Alessandro and Zambon, Daniele and Alippi, Cesare},
      year = {2024},
      month = may,
      number = {arXiv:2405.19933},
      primaryclass = {cs, stat},
      publisher = {arXiv},
      archiveprefix = {arxiv}
    }
- title: 'Temporal Graph ODEs for Irregularly-Sampled Time Series'
  links:
    paper: https://arxiv.org/abs/2404.19508
    code: https://github.com/gravins/TG-ODE
  venue: International Joint Conferences on Artificial Intelligence
  year: 2024
  authors:
  - A. Gravina
  - id:dzambon
  - D. Bacciu
  - id:calippi
  keywords:
    - spatiotemporal graphs
    - graph neural networks
    - irregular sampling
  abstract: Modern graph representation learning works mostly under the assumption of dealing with regularly sampled temporal graph snapshots, which is far from realistic, e.g., social networks and physical systems are characterized by continuous dynamics and sporadic observations. To address this limitation, we introduce the Temporal Graph Ordinary Differential Equation (TG-ODE) framework, which learns both the temporal and spatial dynamics from graph streams where the intervals between observations are not regularly spaced. We empirically validate the proposed approach on several graph benchmarks, showing that TG-ODE can achieve state-of-the-art performance in irregular graph stream tasks.
  bibtex: >
    @inproceedings{gravina2024temporal,
      title = {Temporal Graph {{ODEs}} for Irregularly-Sampled Time Series},
      booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, {IJCAI-24}},
      publisher = {International Joint Conferences on Artificial Intelligence Organization},
      author = {Gravina, Alessio and Zambon, Daniele and Bacciu, Davide and Alippi, Cesare},
      year = {2024}
    }
- title: 'Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations'
  links:
    paper: https://openreview.net/forum?id=CAqdG2dy5s
    code: https://github.com/gdefe/ggnet-virtual-sensing
  venue: International Conference on Learning Representations
  year: 2024
  authors:
  - G. De Felice
  - id:acini
  - id:dzambon
  - V. V. Gusev
  - id:calippi
  keywords:
    - spatiotemporal graphs
    - graph neural networks
    - imputation
    - graph structure learning
  abstract: Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting spatio-temporal measurements coming from physical sensors at different locations. However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to support interpolation. In this paper, we overcome this challenge by leveraging dependencies between the target variable and a set of correlated variables (covariates) that can frequently be associated with each location of interest. From this viewpoint, covariates provide partial observability, and the problem consists of inferring values for unobserved channels by exploiting observations at other locations to learn how such variables can correlate. We introduce a novel graph-based methodology to exploit such relationships and design a graph deep learning architecture, named GgNet, implementing the framework. The proposed approach relies on propagating information over a nested graph structure that is used to learn dependencies between variables as well as locations. GgNet is extensively evaluated under different virtual sensing scenarios, demonstrating higher reconstruction accuracy compared to the state-of-the-art.
  bibtex: >
    @inproceedings{defelice2024graphbased,
      title={Graph-Based {{Virtual Sensing}} from {{Sparse}} and {{Partial Multivariate Observations}}}, 
      booktitle={The {{Twelfth International Conference}} on {{Learning Representations}}}, 
      author={De Felice, Giovanni and Cini, Andrea and Zambon, Daniele and Gusev, Vladimir and Alippi, Cesare},
      year={2024},
      url={https://openreview.net/forum?id=CAqdG2dy5s}
    }
- title: Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling
  links:
    paper: https://arxiv.org/abs/2402.10634
  venue: <i>To appear in</i> International Conference on Machine Learning
  year: 2024
  authors:
  - id:imarisca
  - id:calippi
  - id:fmbianchi
  keywords:
    - spatiotemporal graphs
    - forecasting
  abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate the forecasts. Our approach outperforms state-of-the-art methods on synthetic and real-world benchmarks under different missing data distributions, particularly in the presence of contiguous blocks of missing values.
- title: Graph Deep Learning for Time Series Forecasting
  links:
    paper: https://arxiv.org/abs/2310.15978
  venue: Preprint
  year: 2023
  authors:
  - id:acini
  - id:imarisca
  - id:dzambon
  - id:calippi
  keywords:
    - spatiotemporal graphs
    - forecasting
  abstract: Graph-based deep learning methods have become popular tools to process collections of correlated time series. Differently from traditional multivariate forecasting methods, neural graph-based predictors take advantage of pairwise relationships by conditioning forecasts on a (possibly dynamic) graph spanning the time series collection. The conditioning can take the form of an architectural inductive bias on the neural forecasting architecture, resulting in a family of deep learning models called spatiotemporal graph neural networks. Such relational inductive biases enable the training of global forecasting models on large time-series collections, while at the same time localizing predictions w.r.t. each element in the set (i.e., graph nodes) by accounting for local correlations among them (i.e., graph edges). Indeed, recent theoretical and practical advances in graph neural networks and deep learning for time series forecasting make the adoption of such processing frameworks appealing and timely. However, most of the studies in the literature focus on proposing variations of existing neural architectures by taking advantage of modern deep learning practices, while foundational and methodological aspects have not been subject to systematic investigation. To fill the gap, this paper aims to introduce a comprehensive methodological framework that formalizes the forecasting problem and provides design principles for graph-based predictive models and methods to assess their performance. At the same time, together with an overview of the field, we provide design guidelines, recommendations, and best practices, as well as an in-depth discussion of open challenges and future research directions.
- title: Graph Representation Learning (special session at ESANN 2023)
  links:
    paper: https://doi.org/10.14428/esann/2023.ES2023-4
    doi: https://doi.org/10.14428/esann/2023.ES2023-4
  venue: ESANN
  year: 2023
  authors:
  - D. Bacciu
  - F. Errica
  - A. Micheli
  - N. Navarin
  - L. Pasa
  - M. Podda 
  - id:dzambon
  keywords:
    - graph neural networks
  abstract: In a broad range of real-world machine learning applications, representing examples as graphs is crucial to avoid a loss of information. Due to this in the last few years, the definition of machine learning methods, particularly neural networks, for graph-structured inputs has been gaining increasing attention. In particular, Deep Graph Networks (DGNs) are nowadays the most commonly adopted models to learn a representation that can be used to address different tasks related to nodes, edges, or even entire graphs. This tutorial paper reviews fundamental concepts and open challenges of graph representation learning and summarizes the contributions that have been accepted for publication to the ESANN 2023 special session on the topic.
  bibtex: >
    @inproceedings{bacciu2023graph,
      title = {Graph Representation Learning},
      booktitle = {31st {{European Symposium}} on {{Artificial Neural Networks}}, {{Computational Intelligence}} and {{Machine Learning}}, {{ESANN}} 2023},
      author = {Bacciu, Davide and Errica, Federico and Micheli, Alessio and Navarin, Nicol{\`o} and Pasa, Luca and Podda, Marco and Zambon, Daniele},
      year = {2023},
      pages = {1--10},
      publisher = {i6doc. com},
      doi = {10.14428/esann/2023.ES2023-4},
      isbn = {978-2-87587-088-9}
    }
- title: "A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection"
  links:
    paper: https://arxiv.org/abs/2307.03759
  venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
  year: 2024
  authors:
  - M. Jin
  - H. Y. Koh
  - Q. Wen
  - id:dzambon
  - id:calippi
  - G. I. Webb
  - I. King 
  - S. Pan
  keywords:
    - spatiotemporal graphs
    - graph neural networks
    - forecasting
    - imputation
    - anomaly detection
  abstract: "Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis."
  bibtex: >
    @article{jin2024survey,
      author={Jin, Ming and Koh, Huan Yee and Wen, Qingsong and Zambon, Daniele and Alippi, Cesare and Webb, Geoffrey I. and King, Irwin and Pan, Shirui},
      journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
      title={A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection}, 
      year={2024},
      doi={10.1109/TPAMI.2024.3443141}
    }
- title: Graph-based Time Series Clustering for End-to-End Hierarchical Forecasting
  links:
    paper: https://arxiv.org/abs/2305.19183
  venue: <i>To appear in</i> International Conference on Machine Learning
  year: 2024
  authors:
  - id:acini
  - D. Mandic
  - id:calippi
  keywords:
    - spatiotemporal graphs
    - forecasting
    - time series clustering
    - pooling
  abstract: Existing relationships among time series can be exploited as inductive biases in learning effective forecasting models. In hierarchical time series, relationships among subsets of sequences induce hard constraints (hierarchical inductive biases) on the predicted values. In this paper, we propose a graph-based methodology to unify relational and hierarchical inductive biases in the context of deep learning for time series forecasting. In particular, we model both types of relationships as dependencies in a pyramidal graph structure, with each pyramidal layer corresponding to a level of the hierarchy. By exploiting modern - trainable - graph pooling operators we show that the hierarchical structure, if not available as a prior, can be learned directly from data, thus obtaining cluster assignments aligned with the forecasting objective. A differentiable reconciliation stage is incorporated into the processing architecture, allowing hierarchical constraints to act both as an architectural bias as well as a regularization element for predictions. Simulation results on representative datasets show that the proposed method compares favorably against the state of the art.
- title: Feudal Graph Reinforcement Learning
  links:
    paper: https://arxiv.org/abs/2304.05099
  venue: Preprint
  year: 2023
  authors:
  - id:tmarzi
  - A. Khehra
  - id:acini
  - id:calippi
  first_authors: 2
  keywords:
    - reinforcement learning
    - relational inductive biases
    - graph neural networks
  abstract: We focus on learning composable policies to control a variety of physical agents with possibly different structures. Among state-of-the-art methods, prominent approaches exploit graph-based representations and weight-sharing modular policies based on the message-passing framework. However, as shown by recent literature, message passing can create bottlenecks in information propagation and hinder global coordination. This drawback can become even more problematic in tasks where high-level planning is crucial. In fact, in similar scenarios, each modular policy - e.g., controlling a joint of a robot - would request to coordinate not only for basic locomotion but also achieve high-level goals, such as navigating a maze. A classical solution to avoid similar pitfalls is to resort to hierarchical decision-making. In this work, we adopt the Feudal Reinforcement Learning paradigm to develop agents where control actions are the outcome of a hierarchical (pyramidal) message-passing process. In the proposed Feudal Graph Reinforcement Learning (FGRL) framework, high-level decisions at the top level of the hierarchy are propagated through a layered graph representing a hierarchy of policies. Lower layers mimic the morphology of the physical system and upper layers can capture more abstract sub-modules. The purpose of this preliminary work is to formalize the framework and provide proof-of-concept experiments on benchmark environments (MuJoCo locomotion tasks). Empirical evaluation shows promising results on both standard benchmarks and zero-shot transfer learning settings.
- title: Relational Inductive Biases for Object-Centric Image Generation
  links:
    paper: https://arxiv.org/abs/2303.14681
  venue: Preprint
  year: 2023
  authors:
  - id:lbutera
  - id:acini
  - id:aferrante
  - id:calippi
  keywords:
    - relational inductive biases
    - image generation
    - graph neural networks
  abstract: Conditioning image generation on specific features of the desired output is a key ingredient of modern generative models. Most existing approaches focus on conditioning the generation based on free-form text, while some niche studies use scene graphs to describe the content of the image to be generated. This paper explores novel methods to condition image generation that are based on object-centric relational representations. In particular, we propose a methodology to condition the generation of a particular object in an image on the attributed graph representing its structure and associated style. We show that such architectural biases entail properties that facilitate the manipulation and conditioning of the generative process and allow for regularizing the training procedure. The proposed framework is implemented by means of a neural network architecture combining convolutional operators that operate on both the underlying graph and the 2D grid that becomes the output image. The resulting model learns to generate multi-channel masks of the object that can be used as a soft inductive bias in the downstream generative task. Empirical results show that the proposed approach compares favorably against relevant baselines on image generation conditioned on human poses.
- title: Graph Kalman Filters
  links:
    paper: https://arxiv.org/abs/2303.12021
  venue: Preprint
  year: 2023
  authors:
  - id:calippi
  - id:dzambon
  first_authors: 2
  keywords:
    - spatiotemporal graphs
    - state-space models
    - graph structure learning
  abstract: The well-known Kalman filters model dynamical systems by relying on state-space representations with the next state updated, and its uncertainty controlled, by fresh information associated with newly observed system outputs. This paper generalizes, for the first time in the literature, Kalman and extended Kalman filters to discrete-time settings where inputs, states, and outputs are represented as attributed graphs whose topology and attributes can change with time. The setup allows us to adapt the framework to cases where the output is a vector or a scalar too (node/graph level tasks). Within the proposed theoretical framework, the unknown state-transition and the readout functions are learned end-to-end along with the downstream prediction task.
- title: Taming Local Effects in Graph-based Spatiotemporal Forecasting
  links:
    paper: https://proceedings.neurips.cc/paper_files/paper/2023/hash/ad58c61c71efd5436134a3ecc87da6ea-Abstract-Conference.html
    code: https://github.com/Graph-Machine-Learning-Group/taming-local-effects-stgnns
  venue: Advances in Neural Information Processing Systems
  year: 2023
  authors:
  - id:acini
  - id:imarisca
  - id:dzambon
  - id:calippi
  first_authors: 2
  keywords:
    - spatiotemporal graphs
    - forecasting
    - embeddings
  abstract: Spatiotemporal graph neural networks have shown to be effective in time series forecasting applications, achieving better performance than standard univariate predictors in several settings. These architectures take advantage of a graph structure and relational inductive biases to learn a single (global) inductive model to predict any number of the input time series, each associated with a graph node. Despite the gain achieved in computational and data efficiency w.r.t. fitting a set of local models, relying on a single global model can be a limitation whenever some of the time series are generated by a different spatiotemporal stochastic process. The main objective of this paper is to understand the interplay between globality and locality in graph-based spatiotemporal forecasting, while contextually proposing a methodological framework to rationalize the practice of including trainable node embeddings in such architectures. We ascribe to trainable node embeddings the role of amortizing the learning of specialized components. Moreover, embeddings allow for 1) effectively combining the advantages of shared message-passing layers with node-specific parameters and 2) efficiently transferring the learned model to new node sets. Supported by strong empirical evidence, we provide insights and guidelines for specializing graph-based models to the dynamics of each time series and show how this aspect plays a crucial role in obtaining accurate predictions.
  bibtex: >
    @inproceedings{cini2023taming,
      author = {Cini, Andrea and Marisca, Ivan and Zambon, Daniele and Alippi, Cesare},
      booktitle = {Advances in Neural Information Processing Systems},
      pages = {55375--55393},
      publisher = {Curran Associates, Inc.},
      title = {Taming Local Effects in Graph-based Spatiotemporal Forecasting},
      volume = {36},
      year = {2023}
    }
- title: Where and How to Improve Graph-based Spatio-Temporal Predictors
  links:
    paper: https://arxiv.org/abs/2302.01701
  venue: Preprint
  year: 2023
  authors:
  - id:dzambon
  - id:calippi
  keywords:
    - spatiotemporal graphs
    - residual analysis
  abstract: This paper introduces a novel residual correlation analysis, called AZ-analysis, to assess the optimality of spatio-temporal predictive models. The proposed AZ-analysis constitutes a valuable asset for discovering and highlighting those space-time regions where the model can be improved with respect to performance. The AZ-analysis operates under very mild assumptions and is based on a spatio-temporal graph that encodes serial and functional dependencies in the data; asymptotically distribution-free summary statistics identify existing residual correlation in space and time regions, hence localizing time frames and/or communities of sensors, where the predictor can be improved.
- title: "Peak shaving in distribution networks using stationary energy storage systems: A Swiss case study"
  links:
    paper: https://www.sciencedirect.com/science/article/pii/S2352467723000267
    doi: https://doi.org/10.1016/j.segan.2023.101018
  venue: Sustainable Energy, Grids and Networks, Volume 34
  year: 2023
  bibtex: >
    @article{efkarpidis2023peak,
      title={Peak shaving in distribution networks using stationary energy storage systems: A Swiss case study},
      author={Efkarpidis, Nikolaos A and Imoscopi, Stefano and Geidl, Martin and Cini, Andrea and Lukovic, Slobodan and Alippi, Cesare and Herbst, Ingo},
      journal={Sustainable Energy, Grids and Networks},
      volume={34},
      pages={101018},
      year={2023},
      publisher={Elsevier}
    }
  authors:
  - N. A. Efkarpidis
  - id:simoscopi
  - M. Geidl
  - id:acini
  - id:slukovic
  - id:calippi
  - I. Herbst
  keywords:
    - forecasting
    - energy analytics
  abstract: Grid operators are charged not only by their total energy demand, but also by their highest power demand from the superior grid level. The maximum demand charge is usually imposed on the peak power point of the monthly load profile, hence, shaving demand at peak times is of main concern for the aforesaid stakeholders. In this paper, we present an approach for peak shaving in a distribution grid using a battery energy storage. The developed algorithm is applied and tested with data from a real stationary battery installation at a Swiss utility. This paper proposes a battery storage control scheme that can be used for peak shaving of the total grid load under realistic conditions. Particularly, a rule-based approach combined with a deep-learning load forecasting model is developed and its performance is compared with the theoretical optimum based on real data from the field. The analysis includes both technical and economical results from a simulated storage operation and significant outcomes are given for the application of this method.
- title: Graph State-Space Models
  links:
    paper: https://arxiv.org/abs/2301.01741
  venue: Preprint
  year: 2023
  authors:
  - id:dzambon
  - id:acini
  - id:llivi
  - id:calippi
  keywords:
    - spatiotemporal graphs
    - state-space models
    - graph structure learning
  abstract: State-space models constitute an effective modeling tool to describe multivariate time series and operate by maintaining an updated representation of the system state from which predictions are made. Within this framework, relational inductive biases, e.g., associated with functional dependencies existing among signals, are not explicitly exploited leaving unattended great opportunities for effective modeling approaches. The manuscript aims, for the first time, at filling this gap by matching state-space modeling and spatio-temporal data where the relational information, say the functional graph capturing latent dependencies, is learned directly from data and is allowed to change over time. Within a probabilistic formulation that accounts for the uncertainty in the data-generating process, an encoder-decoder architecture is proposed to learn the state-space model end-to-end on a downstream task. The proposed methodological framework generalizes several state-of-the-art methods and demonstrates to be effective in extracting meaningful relational information while achieving optimal forecasting performance in controlled environments.
- title: Scalable Spatiotemporal Graph Neural Networks
  links:
    paper: https://arxiv.org/abs/2209.06520
    code: https://github.com/Graph-Machine-Learning-Group/sgp
  venue: Proceedings of the AAAI conference on artificial intelligence
  year: 2023
  authors:
  - id:acini
  - id:imarisca
  - id:fmbianchi
  - id:calippi
  first_authors: 2
  keywords:
    - spatiotemporal graphs
    - forecasting
    - reservoir computing
  abstract: Neural forecasting of spatiotemporal time series drives both research
    and industrial innovation in several relevant application domains. Graph neural
    networks (GNNs) are often the core component of the forecasting architecture.
    However, in most spatiotemporal GNNs, the computational complexity scales up to
    a quadratic factor with the length of the sequence times the number of links in
    the graph, hence hindering the application of these models to large graphs and
    long temporal sequences. While methods to improve scalability have been proposed
    in the context of static graphs, few research efforts have been devoted to the
    spatiotemporal case. To fill this gap, we propose a scalable architecture that
    exploits an efficient encoding of both temporal and spatial dynamics. In particular,
    we use a randomized recurrent neural network to embed the history of the input
    time series into high-dimensional state representations encompassing multi-scale
    temporal dynamics. Such representations are then propagated along the spatial
    dimension using different powers of the graph adjacency matrix to generate node
    embeddings characterized by a rich pool of spatiotemporal features. The resulting
    node embeddings can be efficiently pre-computed in an unsupervised manner, before
    being fed to a feed-forward decoder that learns to map the multi-scale spatiotemporal
    representations to predictions. The training procedure can then be parallelized
    node-wise by sampling the node embeddings without breaking any dependency, thus
    enabling scalability to large networks. Empirical results on relevant datasets
    show that our approach achieves results competitive with the state of the art,
    while dramatically reducing the computational burden.
- title: Graph Neural Networks for High-Level Synthesis Design Space Exploration
  venue: ACM Transactions on Design Automation of Electronic Systems
  year: 2022
  links:
    paper: https://dl.acm.org/doi/full/10.1145/3570925
  keywords:
    - graph neural networks
    - design space exploration
    - relational inductive biases
  authors:
  - L. Ferretti
  - id:acini
  - G. Zacharopoulos
  - id:calippi
  - L. Pozzi
  first_authors: 2
  abstract: High-level Synthesis (HLS) Design-Space Exploration (DSE) aims at identifying Pareto-optimal synthesis
   configurations whose exhaustive search is unfeasible due to the design-space dimensionality and the prohibitive 
   computational cost of the synthesis process. Within this framework, we address the design automation problem by 
   proposing graph neural networks that jointly predict acceleration performance and hardware costs of a synthesized 
   behavioral specification given optimization directives. Learned models can be used to rapidly approach the Pareto 
   curve by guiding the DSE, taking into account performance and cost estimates. The proposed method outperforms traditional 
   HLS-driven DSE approaches, by accounting for the arbitrary length of computer programs and the invariant properties of the input. 
   We propose a novel hybrid control and dataflow graph representation that enables training the graph neural network on 
   specifications of different hardware accelerators. Our approach achieves prediction accuracy comparable with that of 
   state-of-the-art simulators without having access to analytical models of the HLS compiler. Finally, the learned representation 
   can be exploited for DSE in unexplored configuration spaces by fine-tuning on a small number of samples from the new target domain. 
   The outcome of the empirical evaluation of this transfer learning shows strong results against state-of-the-art baselines in relevant 
   benchmarks.
- title: Learning to Reconstruct Missing Data from Spatiotemporal Graphs with Sparse
    Observations
  links:
    paper: https://arxiv.org/abs/2205.13479
    code: https://github.com/Graph-Machine-Learning-Group/spin
  venue: Advances in Neural Information Processing Systems
  year: 2022
  authors:
  - id:imarisca
  - id:acini
  - id:calippi
  first_authors: 2
  keywords:
    - spatiotemporal graphs
    - imputation
  abstract: Modeling multivariate time series as temporal signals over a (possibly
    dynamic) graph is an effective representational framework that allows for developing
    models for time series analysis. In fact, discrete sequences of graphs can be
    processed by autoregressive graph neural networks to recursively learn representations
    at each discrete point in time and space. Spatiotemporal graphs are often highly
    sparse, with time series characterized by multiple, concurrent, and even long
    sequences of missing data, e.g., due to the unreliable underlying sensor network.
    In this context, autoregressive models can be brittle and exhibit unstable learning
    dynamics. The objective of this paper is, then, to tackle the problem of learning
    effective models to reconstruct, i.e., impute, missing data points by conditioning
    the reconstruction only on the available observations. In particular, we propose
    a novel class of attention-based architectures that, given a set of highly sparse
    discrete observations, learn a representation for points in time and space by
    exploiting a spatiotemporal diffusion architecture aligned with the imputation
    task. Representations are trained end-to-end to reconstruct observations w.r.t.
    the corresponding sensor and its neighboring nodes. Compared to the state of the
    art, our model handles sparse data without propagating prediction errors or requiring
    a bidirectional model to encode forward and backward time dependencies. Empirical
    results on representative benchmarks show the effectiveness of the proposed method.
- title: 'AZ-whiteness test: a test for uncorrelated noise on spatio-temporal graphs'
  links:
    paper: https://proceedings.neurips.cc/paper_files/paper/2022/hash/4e9fa6e716940a7cfc60c46e6f702f52-Abstract-Conference.html
    code: https://github.com/dzambon/az-whiteness-test
  venue: Advances in Neural Information Processing Systems
  year: 2022
  authors:
  - id:dzambon
  - id:calippi
  keywords:
    - spatiotemporal graphs
    - residual analysis
  abstract: We present the first whiteness test for graphs, i.e., a whiteness test
    for multivariate time series associated with the nodes of a dynamic graph. The
    statistical test aims at finding serial dependencies among close-in-time observations,
    as well as spatial dependencies among neighboring observations given the underlying
    graph. The proposed test is a spatio-temporal extension of traditional tests from
    the system identification literature and finds applications in similar, yet more
    general, application scenarios involving graph signals. The AZ-test is versatile,
    allowing the underlying graph to be dynamic, changing in topology and set of nodes,
    and weighted, thus accounting for connections of different strength, as is the
    case in many application scenarios like transportation networks and sensor grids.
    The asymptotic distribution --- as the number of graph edges or temporal observations
    increases --- is known, and does not assume identically distributed data. We validate
    the practical value of the test on both synthetic and real-world problems, and
    show how the test can be employed to assess the quality of spatio-temporal forecasting
    models by analyzing the prediction residuals appended to the graphs stream.
  bibtex: >
    @inproceedings{zambon2022azwhiteness,
      title = {{{AZ-whiteness}} Test: A Test for Signal Uncorrelation on Spatio-Temporal Graphs},
      booktitle = {Advances in Neural Information Processing Systems},
      author = {Zambon, Daniele and Alippi, Cesare},
      editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
      year = {2022},
      volume = {35},
      pages = {11975--11986},
      publisher = {Curran Associates, Inc.}
    }
- title: Sparse Graph Learning from Spatiotemporal Time Series
  links:
    paper: https://jmlr.org/papers/v24/22-1154.html
    code: https://github.com/andreacini/sparse-graph-learning
  venue: Journal of Machine Learning Research
  year: 2023
  authors:
  - id:acini
  - id:dzambon
  - id:calippi
  keywords:
    - spatiotemporal graphs
    - graph structure learning
    - forecasting
  abstract: Outstanding achievements of graph neural networks for spatiotemporal time series analysis show that relational constraints introduce an effective inductive bias into neural forecasting architectures. Often, however, the relational information characterizing the underlying data-generating process is unavailable and the practitioner is left with the problem of inferring from data which relational graph to use in the subsequent processing stages. We propose novel, principled - yet practical - probabilistic score-based methods that learn the relational dependencies as distributions over graphs while maximizing end-to-end the performance at task. The proposed graph learning framework is based on consolidated variance reduction techniques for Monte Carlo score-based gradient estimation, is theoretically grounded, and, as we show, effective in practice. In this paper, we focus on the time series forecasting problem and show that, by tailoring the gradient estimators to the graph learning problem, we are able to achieve state-of-the-art performance while controlling the sparsity of the learned graph and the computational scalability. We empirically assess the effectiveness of the proposed method on synthetic and real-world benchmarks, showing that the proposed solution can be used as a stand-alone graph identification procedure as well as a graph learning component of an end-to-end forecasting architecture.
  bibtex: >
    @article{cini2023sparse,
      title = {Sparse {{Graph Learning}} from {{Spatiotemporal Time Series}}},
      author = {Cini, Andrea and Zambon, Daniele and Alippi, Cesare},
      year = {2023},
      journal = {Journal of Machine Learning Research},
      volume = {24},
      number = {242},
      pages = {1--36},
      issn = {1533-7928}
    } 
- title: Deep learning for graphs (special session at ESANN 2022)
  links:
    paper: https://doi.org/10.14428/esann/2022.ES2022-7
    doi: https://doi.org/10.14428/esann/2022.ES2022-7
  venue: European Symposium on Artificial Neural Networks, Computational Intelligence
    and Machine Learning
  year: 2022
  keywords:
    - graph neural networks
  authors:
  - D. Bacciu
  - F. Errica
  - N. Navarin
  - L. Pasa
  - id:dzambon
  abstract: The flourishing field of deep learning for graphs relies on the layered
    computation of representations from graph-structured data given as input. A widely
    used strategy for processing graphs is via message passing, based on exchanging
    information among the connected nodes of the graph. Subsequently, node representations
    are employed to address tasks associated with the nodes and edges of a graph,
    or even entire graphs. The present tutorial paper reviews fundamental concepts
    and open challenges of deep learning for graphs and summarizes the contributed
    papers of the ESANN 2022 special session on the topic.
  bibtex: >
    @inproceedings{bacciu2022deep,
      title = {Deep Learning for Graphs},
      booktitle = {30th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, {{ESANN}} 2022},
      author = {Bacciu, Davide and Errica, Federico and Navarin, Nicol{\`o} and Pasa, Luca and Zambon, Daniele},
      year = {2022},
      pages = {481--490},
      doi = {10.14428/esann/2022.ES2022-7},
      isbn = {978-2-87587-084-1},
      organization = {ESANN (i6doc. com)}
    }
- title: Spatio-Temporal Graph Neural Networks for Aggregate Load Forecasting
  venue: IEEE International Joint Conference on Neural Networks
  year: 2022
  links:
    paper: https://ieeexplore.ieee.org/abstract/document/9892780
  authors:
  - S. Eandi
  - id:acini
  - id:slukovic
  - id:calippi
  keywords:
    - spatiotemporal graphs
    - forecasting
    - energy analytics
  abstract: Accurate forecasting of electricity demand is a core component in many
    systems within the modern electricity infrastructure. Several approaches exist
    that tackle this problem by exploiting modern deep learning tools. However, most
    previous works focus on predicting the total load as a univariate time series
    forecasting task, ignoring all fine-grained information captured by the smart
    meters distributed across the power grid. We introduce a methodology to account
    for this information in the graph neural network framework. In particular, we
    consider spatio-temporal graphs where each node is associated with the aggregate
    load of a cluster of smart meters, and a global graph-level attribute indicates
    the total load on the grid. We propose two novel spatio-temporal graph neural
    network models to process this representation and take advantage of both the finer-grained
    information and the relationships existing between the different clusters of meters.
    We compare these models on a widely used, openly available, benchmark against
    a competitive baseline which only accounts for the total load profile. Within
    these settings, we show that the proposed methodology improves forecasting accuracy.
- title: 'Graph iForest: Isolation of anomalous and outlier graphs'
  venue: IEEE International Joint Conference on Neural Networks
  year: 2022
  links:
    paper: https://ieeexplore.ieee.org/document/9892295
    code: https://github.com/dzambon/graph-iforest
    doi: https://doi.org/10.1109/IJCNN55064.2022.9892295
  keywords:
    - anomaly detection
  authors:
  - id:dzambon
  - id:llivi
  - id:calippi
  abstract: "We present an anomaly and outlier detection method for graph data. The
    method relies on the consideration that anomalies and outliers are more easily
    isolated by certain incremental partitionings of the data space. Specifically,
    we build upon the isolation forest method and introduce a new incremental partitioning
    of the space of graphs that makes the isolation forest method applicable to generic
    attributed graphs, i.e., graphs where both nodes and edges can be associated with
    attributes. Within the considered general setup, the topology and the number of
    nodes can change from graph to graph, and a node correspondence between different
    graphs can be absent or unknown. Examples of applications of what proposed include
    the identification of frauds and fake news in communication networks, and breakage
    of systems monitored by sensor networks. The main novel contribution of the paper
    is a graph space partitioning which we prove to be expressive enough to identify
    anomalies and outlier graphs in a given dataset. An empirical analysis
    on synthetic and real-world graphs validates the effectiveness of the proposed
    method."
  bibtex: >
    @inproceedings{zambon2022graph,
      title = {Graph {{iForest}}: {{Isolation}} of Anomalous and Outlier Graphs},
      shorttitle = {Graph {{iForest}}},
      booktitle = {2022 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
      author = {Zambon, Daniele and Livi, Lorenzo and Alippi, Cesare},
      year = {2022},
      month = jul,
      pages = {1--8},
      issn = {2161-4407},
      doi = {10.1109/IJCNN55064.2022.9892295}
    }
- title: Understanding Catastrophic Forgetting of Gated Linear Networks in Continual
    Learning
  venue: IEEE International Joint Conference on Neural Networks
  year: 2022
  keywords:
    - nonstationary environments
    - continual learning
  links:
    paper: https://ieeexplore.ieee.org/document/9892142
    doi: https://doi.org/10.1109/IJCNN55064.2022.9892142
    code: https://github.com/matteo-munari/GrowPGLN
  authors:
  - M. Munari
  - L. Pasa
  - id:dzambon
  - id:calippi
  - N. Navarin
  abstract: In this paper, we consider the recently proposed family of continual learning
    models, called Gated Linear Networks (GLNs), and study two crucial aspects impacting
    on the amount of catastrophic forgetting affecting gated linear networks, namely,
    data standardization and gating mechanism.  Data standardization is particularly
    challenging in the online/continual learning setting because data from future
    tasks is not available beforehand. The results obtained using an online standardization
    method show a considerably higher amount of forgetting compared to an offline
    --static-- standardization.  Interestingly, with the latter standardization, we
    observe that GLNs show almost no forgetting on the considered benchmark datasets.  Secondly,
    for an effective GLNs, it is essential to tailor the hyperparameters of the gating
    mechanism to the data distribution. In this paper, we propose a gating strategy
    based on a set of prototypes and the resulting Voronoi tessellation. The experimental
    assessment shows that the proposed approach is more robust to different data standardizations
    compared to the original one, based on a halfspace gating mechanism, and shows
    improved predictive performance.
  bibtex: >
    @inproceedings{munari2022understanding,
      title = {Understanding {{Catastrophic Forgetting}} of {{Gated Linear Networks}} in {{Continual Learning}}},
      booktitle = {2022 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
      author = {Munari, Matteo and Pasa, Luca and Zambon, Daniele and Alippi, Cesare and Navarin, Nicol{\`o}},
      year = {2022},
      month = jul,
      pages = {1--8},
      issn = {2161-4407},
      doi = {10.1109/IJCNN55064.2022.9892142}
    }
- title: Understanding Pooling in Graph Neural Networks
  links:
    paper: https://arxiv.org/abs/2110.05292
    doi: https://doi.org/10.1109/TNNLS.2022.3190922
    code: https://github.com/danielegrattarola/SRC
  venue: IEEE Transactions on Neural Networks and Learning Systems
  year: 2024
  authors:
  - id:dgrattarola
  - id:dzambon
  - id:fmbianchi
  - id:calippi
  keywords:
    - graph neural networks
    - pooling
  abstract: Inspired by the conventional pooling layers in convolutional neural networks,
    many recent works in the field of graph machine learning have introduced pooling
    operators to reduce the size of graphs. The great variety in the literature stems
    from the many possible strategies for coarsening a graph, which may depend on
    different assumptions on the graph structure or the specific downstream task.
    In this paper we propose a formal characterization of graph pooling based on three
    main operations, called selection, reduction, and connection, with the goal of
    unifying the literature under a common framework. Following this formalization,
    we introduce a taxonomy of pooling operators and categorize more than thirty pooling
    methods proposed in recent literature. We propose criteria to evaluate the performance
    of a pooling operator and use them to investigate and contrast the behavior of
    different classes of the taxonomy on a variety of tasks.
  bibtex: >
    @article{grattarola2024understanding,
      title = {Understanding {{Pooling}} in {{Graph Neural Networks}}},
      author = {Grattarola, Daniele and Zambon, Daniele and Bianchi, Filippo Maria and Alippi, Cesare},
      year = {2024},
      month = feb,
      journal = {IEEE Transactions on Neural Networks and Learning Systems},
      volume = {35},
      number = {2},
      pages = {2708--2718},
      doi = {10.1109/TNNLS.2022.3190922}
    }
- title: 'Deep Learning for Time Series Forecasting: The Electric Load Case'
  links:
    paper: https://arxiv.org/abs/1907.09207
  venue: CAAI Transactions on Intelligence Technology
  year: 2022
  authors:
  - id:agasparin
  - id:slukovic
  - id:calippi
  keywords:
    - forecasting
    - energy analytics
  abstract: Management and efficient operations in critical infrastructure such as
    Smart Grids take huge advantage of accurate power load forecasting which, due
    to its nonlinear nature, remains a challenging task. Recently, deep learning has
    emerged in the machine learning field achieving impressive performance in a vast
    range of tasks, from image classification to machine translation. Applications
    of deep learning models to the electric load forecasting problem are gaining interest
    among researchers as well as the industry, but a comprehensive and sound comparison
    among different architectures is not yet available in the literature. This work
    aims at filling the gap by reviewing and experimentally evaluating on two real-world
    datasets the most recent trends in electric load forecasting, by contrasting deep
    learning architectures on short term forecast (one day ahead prediction). Specifically,
    we focus on feedforward and recurrent neural networks, sequence to sequence models
    and temporal convolutional neural networks along with architectural variants,
    which are known in the signal processing community but are novel to the load forecasting
    one.
- title: Seizure localisation with attention-based graph neural networks
  links:
    paper: https://www.biorxiv.org/content/10.1101/2020.12.03.409979v1.abstract
    doi: https://doi.org/10.1016/j.eswa.2022.117330
  venue: Expert Systems with Applications
  year: 2022
  authors:
  - id:dgrattarola
  - id:llivi
  - id:calippi
  - R. Wennberg
  - T. A. Valiante
  abstract: "Graph neural networks (GNNs) and the attention mechanism are two of the
    most significant advances in artificial intelligence methods over the past few
    years. The former are neural networks able to process graph-structured data, while
    the latter learns to selectively focus on those parts of the input that are more
    relevant for the task at hand. In this paper, we propose a methodology for seizure
    localisation which combines the two approaches. Our method is composed
    of several blocks. First, we represent brain states in a compact way by computing
    functional networks from intracranial electroencephalography recordings, using
    metrics to quantify the coupling between the activity of different brain areas.
    Then, we train a GNN to correctly distinguish between functional networks associated
    with interictal and ictal phases. The GNN is equipped with an attention-based
    layer which automatically learns to identify those regions of the brain (associated
    with individual electrodes) that are most important for a correct classification.
    The localisation of these regions is fully unsupervised, meaning that it does
    not use any prior information regarding the seizure onset zone. We
    report results both for human patients and for simulators of brain activity. We
    show that the regions of interest identified by the GNN strongly correlate with
    the localisation of the seizure onset zone reported by electroencephalographers.
    We also show that our GNN exhibits uncertainty on those patients for which the
    clinical localisation was also unsuccessful, highlighting the robustness of the
    proposed approach."
  bibtex: >
    @article{GRATTAROLA2022117330,
      title = {Seizure localisation with attention-based graph neural networks},
      journal = {Expert Systems with Applications},
      volume = {203},
      pages = {117330},
      year = {2022},
      issn = {0957-4174},
      doi = {10.1016/j.eswa.2022.117330},
      author = {Daniele Grattarola and Lorenzo Livi and Cesare Alippi and Richard Wennberg and Taufik A. Valiante},
    }
- title: 'Learning dynamical systems using dynamical systems: the reservoir computing
    approach'
  links:
    paper: https://susi.usi.ch/usi/documents/319318
  venue: PhD Thesis, Universit della Svizzera italiana
  year: 2022
  authors:
  - id:pverzelli
  keywords:
    - reservoir computing
  abstract: 'Dynamical systems have been used to describe a vast range of phenomena,
    including physical sciences, biology, neurosciences, and economics just to name
    a few. The development of a mathematical theory for dynamical systems allowed
    researchers to create precise models of many phenomena, predicting their behaviors
    with great accuracy. For many challenges of dynamical systems, highly accurate
    models are notably hard to produce due to the enormous number of variables involved
    and the complexity of their interactions. Yet, in recent years the availability
    of large datasets has driven researchers to approach these complex systems with
    machine learning techniques. These techniques are valuable in settings where no
    model can be formulated explicitly, but not rarely the working principles of these
    models are obscure and their optimization is driven by heuristics. In this context,
    this work aims at advancing the field by opening the black-box of data-driven
    models developed for dynamical systems. We focus on Recurrent Neural Networks
    (RNNs), one of the most promising and yet less understood approaches. In particular,
    we concentrate on a specific neural architecture that goes under the name of Reservoir
    Computing (RC). We address three problems: (1) how the learning procedure of these
    models can be understood and improved, (2) how these systems encode a representation
    of the inputs they receive, and (3) how the dynamics of these systems affect their
    performance. We make use of various tools taken from the theory of dynamical systems
    to explain how we can better understand the working principles of RC in dynamical
    systems, aiming at developing new guiding principles to improve their design.'
- title: Anomaly and Change Detection in Sequences of Graphs
  links:
    paper: https://dzambon.github.io/
  venue: PhD Thesis, Universit della Svizzera italiana
  year: 2021
  keywords:
    - nonstationary environments
    - anomaly detection
    - change detection
  authors:
    - id:dzambon
  abstract: "We are experiencing a significant increase in the amount of data collected
    by sensor and social networks, thanks to advances in technology and the spread
    of social platforms. Doing inference on such huge data streams is a crucial task
    both to academia and industry. As data streams coming from sensors --either physical
    or virtual-- generally share functional dependencies, graphs emerge as rich structures
    able to model both information at the sensor/entity level and the complex relationships
    existing among entities. In turn, this graph representation enables us to do inference
    on graph streams, e.g., through graph neural networks and geometric deep learning.
    However, in general, such processing techniques assume the stationarity hypothesis,
    which is not always granted, e.g., whenever we experience sensor aging, time variance
    or changes in the users' preferences on social platforms. In this doctoral thesis,
    we address the problem of identifying changes in stationarity caused by unknown
    phenomena occurring in the underlying data-generating process and emerging in
    the sequence of graphs. Scientific outcomes permit us to also address the anomaly
    detection problem that, indeed, represents a valuable follow-up of the research.
    We consider a general family of attributed graphs with non-identified vertices
    in order to cover the broadest spectrum of applications. The major contribution
    of the thesis resides in a methodology for processing a sequence of graphs to
    detect unexpected events (changes in stationarities and/or occurrence of anomalies)
    in the data generating process. The methodology relies on the design of novel
    graph-level embeddings and change detection methods supported by a solid theoretical
    framework."
- title: 'Graph neural networks: operators and architectures'
  links:
    paper: https://susi.usi.ch/usi/documents/319253
  venue: PhD Thesis, Universit della Svizzera italiana
  year: 2021
  keywords:
    - graph neural networks
  authors:
  - id:dgrattarola
  abstract: 'This thesis explores the field of graph neural networks, a class of deep
    learning models designed to learn representations of graphs. We organise the work
    into two parts. In the first part, we focus on the essential building blocks of
    graph neural networks. We present three novel operators for learning graph representations:
    one graph convolutional layer and two methods for pooling. We put particular emphasis
    on the topic of pooling, introducing a universal and modular framework to describe
    pooling operators, a taxonomy to organise the literature, and a set of evaluation
    criteria to assess an operators performance. The second part focuses on specific
    graph neural network architectures and their applications to cutting-edge problems
    in dynamical systems and computational biology. We present three main contributions.
    First, we introduce an autoencoder architecture for learning graph representations
    in non-Euclidean spaces. We apply our model to the tasks of molecule generation
    and change detection in graph sequences. Second, we propose a graph neural network
    designed to be interpretable, specifically to solve the problem of seizure localisation
    in subjects with epilepsy. Finally, we discuss the design of autoregressive models
    for sequences of graphs.'
- title: Learning Graph Cellular Automata
  links:
    paper: https://proceedings.neurips.cc/paper/2021/hash/af87f7cdcda223c41c3f3ef05a3aaeea-Abstract.html
    code: https://github.com/danielegrattarola/GNCA
  venue: Advances in Neural Information Processing Systems
  year: 2021
  authors:
  - id:dgrattarola
  - id:llivi
  - id:calippi
  abstract: "Cellular automata (CA) are a class of computational models that exhibit
    rich dynamics emerging from the local interaction of cells arranged in a regular
    lattice. In this work we focus on a generalised version of typical
    CA, called graph cellular automata (GCA), in which the lattice structure is replaced
    by an arbitrary graph. In particular, we extend previous work that
    used convolutional neural networks to learn the transition rule of conventional
    CA and we use graph neural networks to learn a variety of transition rules for
    GCA. First, we present a general-purpose architecture for learning
    GCA, and we show that it can represent any arbitrary GCA with finite and discrete
    state space. Then, we test our approach on three different tasks:
    1) learning the transition rule of a GCA on a Voronoi tessellation; 2) imitating
    the behaviour of a group of flocking agents; 3) learning a rule that converges
    to a desired target state."
  bibtex: >
    @inproceedings{NEURIPS2021_af87f7cd,
      author = {Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
      pages = {20983--20994},
      publisher = {Curran Associates, Inc.},
      title = {Learning Graph Cellular Automata},
      volume = {34},
      year = {2021}
    }
- title: Gaussian Approximation for Bias Reduction in Q-Learning
  links:
    paper: https://jmlr.org/papers/v22/20-633.html
  venue: Journal of Machine Learning Research
  year: 2021
  authors:
  - C. D'Eramo
  - id:acini
  - A. Nuara
  - M. Pirotta
  - id:calippi
  - J. Peters
  - M. Restelli
  first_authors: 2
  keywords:
    - reinforcement learning
  abstract: Temporal-Difference off-policy algorithms are among the building blocks
    of reinforcement learning (RL). Within this family, Q-Learning is arguably the
    most famous one, which has been widely studied and extended. The update rule of
    Q-learning involves the use of the maximum operator to estimate the maximum expected
    value of the return. However, this estimate is positively biased, and may hinder
    the learning process, especially in stochastic environments and when function
    approximation is used. We introduce the Weighted Estimator as an effective solution
    to mitigate the negative effects of overestimation in Q-Learning. The Weighted
    Estimator estimates the maximum expected value as a weighted sum of the action
    values, with the weights being the probabilities that each action value is the
    maximum. In this work, we study the problem from the statistical perspective of
    estimating the maximum expected value of a set of random variables and provide
    bounds to the bias and the variance of the Weighted Estimator, showing its advantages
    over other estimators present in literature. Then, we derive algorithms to enable
    the use of the Weighted Estimator, in place of the Maximum Estimator, in online
    and batch RL, and we introduce a novel algorithm for deep RL. Finally, we empirically
    evaluate our algorithms in a large set of heterogeneous problems, encompassing
    discrete and continuous, low and high dimensional, deterministic and stochastic
    environments. Experimental results show the effectiveness of the Weighted Estimator
    in controlling the bias of the estimate, resulting in better performance than
    representative baselines and robust learning w.r.t. a large set of diverse environments.
- title: 'Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural
    Networks'
  links:
    paper: https://arxiv.org/abs/2108.00298
    code: https://github.com/Graph-Machine-Learning-Group/grin
  venue: International Conference on Learning Representations
  year: 2022
  authors:
  - id:acini
  - id:imarisca
  - id:calippi
  first_authors: 2
  keywords:
    - spatiotemporal graphs
    - imputation
  abstract: Dealing with missing values and incomplete time series is a labor-intensive,
    tedious, inevitable task when handling data coming from real-world applications.
    Effective spatio-temporal representations would allow imputation methods to reconstruct
    missing temporal data by exploiting information coming from sensors at different
    locations. However, standard methods fall short in capturing the nonlinear time
    and space dependencies existing within networks of interconnected sensors and
    do not take full advantage of the available - and often strong - relational information.
    Notably, most state-of-the-art imputation methods based on deep learning do not
    explicitly model relational aspects and, in any case, do not exploit processing
    frameworks able to adequately represent structured spatio-temporal data. Conversely,
    graph neural networks have recently surged in popularity as both expressive and
    scalable tools for processing sequential data with relational inductive biases.
    In this work, we present the first assessment of graph neural networks in the
    context of multivariate time series imputation. In particular, we introduce a
    novel graph neural network architecture, named GRIN, which aims at reconstructing
    missing data in the different channels of a multivariate time series by learning
    spatio-temporal representations through message passing. Empirical results show
    that our model outperforms state-of-the-art methods in the imputation task on
    relevant real-world benchmarks with mean absolute error improvements often higher
    than 20%.
- title: Deep learning for graphs (special session at ESANN 2021)
  links:
    paper: https://doi.org/10.14428/esann/2021.ES2021-5
    doi: https://doi.org/10.14428/esann/2021.ES2021-5
  venue: European Symposium on Artificial Neural Networks
  year: 2021
  keywords:
    - graph neural networks
  authors:
  - D. Bacciu
  - id:fmbianchi
  - B. Paassen
  - id:calippi
  abstract: Deep learning for graphs encompasses all those models endowed with multiple
    layers of abstraction, which operate on data represented as graphs. The most common
    building blocks of these models are graph encoding layers, which compute a vector
    embedding for each node in a graph based on a sum of messages received from its
    neighbors. However, the family also includes architectures with decoders from
    vectors to graphs and models that process time-varying graphs and hypergraphs.
    In this paper, we provide an overview of the key concepts in the field, point
    towards open questions, and frame the contributions of the ESANN 2021 special
    session into the broader context of deep learning for graphs.
- title: 'Precise Agriculture: Effective Deep Learning Strategies to Detect Pest Insects'
  links:
    paper: https://www.ieee-jas.net/fileZDHXBEN/journal/article/zdhxbywb/newcreate/JAS-2021-0798.pdf
  venue: Journal of Automatica Sinica
  year: 2021
  authors:
  - id:lbutera
  - id:aferrante
  - M. Jermini
  - M. Prevostini
  - id:calippi
  abstract: Pest  insect  monitoring  and  control  is  crucial  toensure a safe and
    profitable crop growth in all plantation types, aswell as guarantee food quality
    and limited use of pesticides. Weaim  at  extending  traditional  monitoring  by  means  of  traps,  byinvolving
    the general public in reporting the presence of insectsby  using  smartphones.  This  includes  the  largely  unexploredproblem  of  detecting  insects  in  images  that  are  taken  in  non-controlled  conditions.  Furthermore,  pest  insects  are,  in  manycases,  extremely  similar  to  other  species  that  are  harmless.Therefore,  computer  vision  algorithms  must  not  be  fooled  bythese  similar  insects,  not  to  raise  unmotivated  alarms.  In  thiswork,  we  study  the  capabilities  of  state-of-the-art  (SoA)  objectdetection
    models based on convolutional neural networks (CNN)for  the  task  of  detecting  beetle-like  pest  insects  on  non-homogeneous  images  taken  outdoors  by  different  sources.Moreover,
    we focus on disambiguating a pest insect from similarharmless species. We consider
    not only detection performance ofdifferent models, but also required computational
    resources. Thisstudy aims at providing a baseline model for this kind of tasks.Our
    results show the suitability of current SoA models for thisapplication, highlighting
    how FasterRCNN with a MobileNetV3backbone is a particularly good starting point
    for accuracy andinference execution latency. This combination provided a meanaverage  precision  score  of  92.66%  that  can  be  consideredqualitatively  at  least  as  good  as  the  score  obtained  by  otherauthors
    that adopted more specific models.
- title: Graph Edit Networks
  links:
    paper: https://openreview.net/forum?id=dlEJsyHGeaL
    code: https://gitlab.com/bpaassen/graph-edit-networks
  venue: International Conference on Learning Representations
  year: 2021
  keywords:
    - graph neural networks
  authors:
  - B. Paassen
  - id:dgrattarola
  - id:dzambon
  - id:calippi
  - B. Hammer
  abstract: While graph neural networks have made impressive progress in classification
    and regression on graphs, few approaches to date perform time series prediction
    on graphs and those that do are mostly limited to edge changes. We suggest that
    graph edits are a more natural interface for graph-to-graph learning. In particular,  graph
    edits are general enough to describe any graph-to-graph change, not only edge
    changes, they are sparse, making them easier to understand for humans and more
    efficient computationally, and they are local, avoiding the need for pooling layers
    in graph neural networks. In this paper, we propose a simple linear layer - the
    graph edit network - which takes node embeddings as input and generates a sequence
    of graph edits that transform the input graph to the output graph. Theoretically,
    we show that a mapping between the node sets of two graphs is sufficient to construct
    training data for a graph edit network and that an optimal mapping yields edit
    scripts that are almost as short as the graph edit distance between the graphs.
    We further provide a proof-of-concept.
  bibtex: >
    @inproceedings{paassen2021graph,
      title = {Graph Edit Networks},
      booktitle = {International Conference on Learning Representations ({{ICLR}})},
      author = {Paassen, Benjamin and Grattarola, Daniele and Zambon, Daniele and Alippi, Cesare and Hammer, Barbara Eva},
      year = {2021}
      url={https://openreview.net/forum?id=dlEJsyHGeaL}
    }
- title: Graph Neural Networks in TensorFlow and Keras with Spektral
  links:
    paper: https://arxiv.org/abs/2006.12138
    doi: https://doi.org/10.1109/MCI.2020.3039072
    code: https://github.com/danielegrattarola/spektral/
  venue: IEEE Computational Intelligence Magazine
  year: 2021
  authors:
  - id:dgrattarola
  - id:calippi
  keywords:
    - graph neural networks
    - library
  abstract: Graph neural networks have enabled the application of deep learning to
    problems that can be described by graphs, which are found throughout the different
    fields of science, from physics to biology, natural language processing, telecommunications
    or medicine. In this paper we present Spektral, an open-source Python library
    for building graph neural networks with TensorFlow and the Keras application programming
    interface. Spektral implements a large set of methods for deep learning on graphs,
    including message-passing and pooling operators, as well as utilities for processing
    graphs and loading popular benchmark datasets. The purpose of this library is
    to provide the essential building blocks for creating graph neural networks, focusing
    on the guiding principles of user-friendliness and quick prototyping on which
    Keras is based. Spektral is, therefore, suitable for absolute beginners and expert
    deep learning practitioners alike. In this work, we present an overview of Spektral's
    features and report the performance of the methods implemented by the library
    in scenarios of node classification, graph classification, and graph regression.
  bibtex: >
    @ARTICLE{9321429,
      author={Grattarola, Daniele and Alippi, Cesare},
      journal={IEEE Computational Intelligence Magazine}, 
      title={Graph Neural Networks in TensorFlow and Keras with Spektral [Application Notes]}, 
      year={2021},
      volume={16},
      number={1},
      pages={99-106},
      keywords={Graph neural networks;Predictive models;Biological system modeling;Computational modeling;Deep learning;Benchmark testing},
      doi={10.1109/MCI.2020.3039072}
    }
- title: Learn to Synchronize, Synchronize to Learn
  links:
    paper: https://arxiv.org/abs/2010.02860
  venue: 'Chaos: An Interdisciplinary Journal of Nonlinear Science'
  year: 2021
  authors:
  - id:pverzelli
  - id:calippi
  - id:llivi
  keywords:
    - reservoir computing
  abstract: In recent years, the machine learning community has seen a continuous
    growing interest in research aimed at investigating dynamical aspects of both
    training procedures and perfected models. Of particular interest among recurrent
    neural networks, we have the Reservoir Computing (RC) paradigm for its conceptual
    simplicity and fast training scheme. Yet, the guiding principles under which RC
    operates are only partially understood. In this work, we study the properties
    behind learning dynamical systems with RC and propose a new guiding principle
    based on Generalized Synchronization (GS) granting its feasibility. We show that
    the well-known Echo State Property (ESP) implies and is implied by GS, so that
    theoretical results derived from the ESP still hold when GS does. However, by
    using GS one can profitably study the RC learning procedure by linking the reservoir
    dynamics with the readout training. Notably, this allows us to shed light on the
    interplay between the input encoding performed by the reservoir and the output
    produced by the readout optimized for the task at hand. In addition, we show that
    - as opposed to the ESP - satisfaction of the GS can be measured by means of the
    Mutual False Nearest Neighbors index, which makes effective to practitioners theoretical
    derivations.
- title: Input-to-State Representation in Linear Reservoir Dynamics
  links:
    paper: https://arxiv.org/abs/2003.10585
  venue: IEEE Transactions on Neural Networks and Learning Systems
  year: 2021
  authors:
  - id:pverzelli
  - id:calippi
  - id:llivi
  - P. Tino
  keywords:
    - reservoir computing
  abstract: 'Reservoir computing is a popular approach to design recurrent neural
    networks, due to its training simplicity and its approximation performance. The
    recurrent part of these networks is not trained (e.g. via gradient descent), making
    them appealing for analytical studies, raising the interest of a vast community
    of researcher spanning from dynamical systems to neuroscience. It emerges that,
    even in the simple linear case, the working principle of these networks is not
    fully understood and the applied research is usually driven by heuristics. A novel
    analysis of the dynamics of such networks is proposed, which allows one to express
    the state evolution using the controllability matrix. Such a matrix encodes salient
    characteristics of the network dynamics: in particular, its rank can be used as
    an input-indepedent measure of the memory of the network. Using the proposed approach,
    it is possible to compare different architectures and explain why a cyclic topology
    achieves favourable results.'
- title: Graph Neural Networks with Convolutional ARMA Filters
  links:
    paper: https://arxiv.org/abs/1901.01343
    code: https://github.com/danielegrattarola/spektral/blob/master/examples/node_prediction/citation_arma.py
  keywords: 
    - graph neural networks
  venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
  year: 2021
  authors:
  - id:fmbianchi
  - id:dgrattarola
  - id:llivi
  - id:calippi
  abstract: We propose a novel graph convolutional layer based on auto-regressive
    moving average (ARMA) filters that, compared to the polynomial ones, provide a
    more flexible response thanks to a rich transfer function that accounts for the
    concept of state. We implement the ARMA filter with a recursive and distributed
    formulation, obtaining a convolutional layer that is efficient to train, is localized
    in the node space and can be applied to graphs with different topologies.
- title: Deep Reinforcement Learning with Weighted Q-Learning
  links:
    paper: https://arxiv.org/abs/2003.09280
  venue: The Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM)
  year: 2022
  authors:
  - id:acini
  - C. D'Eramo
  - J. Peters
  - id:calippi
  keywords:
    - reinforcement learning
  abstract: Overestimation of the maximum action-value is a well-known problem that
    hinders Q-Learning performance, leading to suboptimal policies and unstable learning.
    Among several Q-Learning variants proposed to address this issue, Weighted Q-Learning
    (WQL) effectively reduces the bias and shows remarkable results in stochastic
    environments. WQL uses a weighted sum of the estimated action-values, where the
    weights correspond to the probability of each action-value being the maximum;
    however, the computation of these probabilities is only practical in the tabular
    settings. In this work, we provide the methodological advances to benefit from
    the WQL properties in Deep Reinforcement Learning (DRL), by using neural networks
    with Dropout Variational Inference as an effective approximation of deep Gaussian
    processes. In particular, we adopt the Concrete Dropout variant to obtain calibrated
    estimates of epistemic uncertainty in DRL. We show that model uncertainty in DRL
    can be useful not only for action selection, but also action evaluation. We analyze
    how the novel Weighted Deep Q-Learning algorithm reduces the bias wrt relevant
    baselines and provide empirical evidence of its advantages on several representative
    benchmarks.
- title: Hierarchical Representation Learning in Graph Neural Networks with Node Decimation
    Pooling
  links:
    paper: https://arxiv.org/abs/1910.11436
    code: https://github.com/danielegrattarola/decimation-pooling
  venue: IEEE Transactions on Neural Networks and Learning Systems
  year: 2020
  authors:
  - id:fmbianchi
  - id:dgrattarola
  - id:llivi
  - id:calippi
  keywords:
    - graph neural networks
    - pooling
  abstract: We propose Node Decimation Pooling (NDP), a pooling operator for GNNs
    that generates coarsened versions of a graph by leveraging on its topology only.
    During training, the GNN learns new representations for the vertices and fits
    them to a pyramid of coarsened graphs, which is computed in a pre-processing step.
    As theoretical contributions, we first demonstrate the equivalence between the
    MAXCUT partition and the node decimation procedure on which NDP is based. Then,
    we propose a procedure to sparsify the coarsened graphs for reducing the computational
    complexity in the GNN; we also demonstrate that it is possible to drop many edges
    without significantly altering the graph spectra of coarsened graphs.
- title: Graph Random Neural Features for Distance-Preserving Graph Representations
  links:
    paper: http://proceedings.mlr.press/v119/zambon20a.html
    code: https://github.com/dzambon/graph-random-neural-features
  venue: International Conference on Machine Learning
  year: 2020
  keywords:
    - graph neural networks
    - embeddings
  authors:
  - id:dzambon
  - id:calippi
  - id:llivi
  abstract: "Graph Random Neural Features (GRNF) is a novel embedding method from
    graph-structured data to real vectors based on a family of graph neural networks.
    GRNF can be used within traditional processing methods or as a training-free input
    layer of a graph neural network. The theoretical guarantees
    that accompany GRNF ensure that the considered graph distance is metric, hence
    allowing to distinguish any pair of non-isomorphic graphs, and that GRNF approximately
    preserves its metric structure."
  bibtex: >
    @inproceedings{zambon2020graph,
      title = {Graph {{Random Neural Features}} for {{Distance-Preserving Graph Representations}}},
      booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
      author = {Zambon, Daniele and Alippi, Cesare and Livi, Lorenzo},
      editor = {Daum{\`e} III, Hal and Singh, Aarti},
      year = {2020},
      month = nov,
      pages = {10968--10977},
      publisher = {PMLR},
      issn = {2640-3498}
    }
- title: Spectral Clustering with Graph Neural Networks for Graph Pooling
  links:
    paper: https://arxiv.org/abs/1907.00481
    code: https://github.com/FilippoMB/Spectral-Clustering-with-Graph-Neural-Networks-for-Graph-Pooling
  venue: International Conference on Machine Learning
  year: 2020
  authors:
  - id:fmbianchi
  - id:dgrattarola
  - id:calippi
  keywords:
    - graph neural networks
    - pooling
  abstract: We propose a graph clustering approach that addresses some limitations
    of the spectral clustering algorithm. We formulate a continuous relaxation of
    the normalized minCUT problem and train a GNN to compute cluster assignments that
    minimize this objective. Our GNN-based implementation is differentiable, does
    not require to compute the spectral decomposition, and learns a clustering function
    that can be quickly evaluated on out-of-sample graphs. From the proposed clustering
    method, we design a graph pooling operator that overcomes some important limitations
    of state-of-the-art graph pooling techniques and achieves the best performance
    in several supervised and unsupervised tasks.
- title: Cluster-based Aggregate Load Forecasting with Deep Neural Networks
  links:
    paper: https://ieeexplore.ieee.org/document/9207503
  venue: IEEE International Joint Conference on Neural Networks
  year: 2020
  authors:
  - id:acini
  - id:slukovic
  - id:calippi
  keywords:
    - forecasting
    - energy analytics
  abstract: Highly accurate power demand forecasting represents one of key challenges
    of Smart Grid applications. In this setting, a large number of Smart Meters produces
    huge amounts of data that need to be processed to predict the load requested by
    the grid. Due to the high dimensionality of the problem, this often results in
    the adoption of simple aggregation strategies for the power that fail in capturing
    the relational information existing among the different types of user. A possible
    alternative, known as Cluster-based Aggregate Forecasting, consists in clustering
    the load profiles and, on top of that, building predictors of the aggregate at
    the cluster-level. In this work we explore the technique in the context of predictors
    based on deep recurrent neural networks and address the scalability issues presenting
    neural architectures adequate to process cluster-level aggregates. The proposed
    methods are finally evaluated both on a publicly available benchmark and a heterogenous
    dataset of Smart Meter data from an entire, medium-sized, Swiss town.
- title: Echo State Networks with Self-Normalizing Activations on the Hyper-Sphere
  links:
    paper: https://arxiv.org/abs/1903.11691
  venue: Nature Scientific Reports
  year: 2019
  authors:
  - id:pverzelli
  - id:calippi
  - id:llivi
  keywords:
    - reservoir computing
  abstract: We propose a model of echo state networks that eliminates critical dependence
    on hyper-parameters, resulting in networks that provably cannot enter a chaotic
    regime and, at the same time, denotes nonlinear behavior in phase space characterized
    by a large memory of past inputs, comparable to the one of linear networks. Our
    contribution is supported by experiments corroborating our theoretical findings,
    showing that the proposed model displays dynamics that are rich-enough to approximate
    many common nonlinear systems used for benchmarking.
- title: Autoregressive Models for Sequences of Graphs
  links:
    paper: https://arxiv.org/abs/1903.07299
    doi: https://doi.org/10.1109/IJCNN.2019.8852131
    code: https://github.com/dzambon/NGAR
  venue: IEEE International Joint Conference on Neural Networks
  year: 2019
  keywords:
    - graph neural networks
  authors:
  - id:dzambon
  - id:dgrattarola
  - id:llivi
  - id:calippi
  abstract: This paper proposes an autoregressive (AR) model for sequences of graphs,
    which generalizes traditional AR models. A first novelty consists in formalizing
    the AR model for a very general family of graphs, characterized by a variable
    topology, and attributes associated with nodes and edges. A graph neural network
    is also proposed to learn the AR function associated with the graph-generating
    process, and subsequently predict the next graph in a sequence.
  bibtex: >
    @inproceedings{zambon2020graph,
      title = {Graph {{Random Neural Features}} for {{Distance-Preserving Graph Representations}}},
      booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
      author = {Zambon, Daniele and Alippi, Cesare and Livi, Lorenzo},
      editor = {Daum{\`e} III, Hal and Singh, Aarti},
      year = {2020},
      month = nov,
      pages = {10968--10977},
      publisher = {PMLR},
      issn = {2640-3498}
    }
- title: Adversarial Autoencoders with Constant-Curvature Latent Manifolds
  links:
    paper: https://arxiv.org/abs/1812.04314
    code: https://github.com/danielegrattarola/ccm-aae
  venue: Applied Soft Computing
  year: 2019
  keywords:
    - graph neural networks
  authors:
  - id:dgrattarola
  - id:llivi
  - id:calippi
  abstract: We introduce the constant-curvature manifold adversarial autoencoder (CCM-AAE),
    a probabilistic generative model trained to represent a data distribution on a
    constant-curvature Riemannian manifold (CCM). Our method works by matching the
    aggregated posterior of the CCM-AAE with a probability distribution defined on
    a CCM, so that the encoder implicitly learns to represent data on the CCM to fool
    the discriminator network. The geometric constraint is also explicitly imposed
    by jointly training the CCM-AAE to maximize the membership degree of the embeddings
    to the CCM.
- title: Change-Point Methods on a Sequence of Graphs
  links:
    paper: https://arxiv.org/abs/1805.07113
    doi: https://doi.org/10.1109/TSP.2019.2953596
    code: https://github.com/dzambon/cpm-graph-sequence
  venue: IEEE Transactions on Signal Processing
  year: 2019
  authors:
  - id:dzambon
  - id:calippi
  - id:llivi
  keywords:
    - nonstationary environments
    - change detection
  abstract: Given a finite sequence of graphs, we propose a methodology to identify
    possible changes in stationarity in the stochastic process that generated such
    graphs. We consider a general family of attributed graphs for which both topology
    (vertices and edges) and associated attributes are allowed to change over time,
    without violating the stationarity hypothesis. Novel Change-Point Methods (CPMs)
    are proposed that map graphs onto vectors, apply a suitable statistical test in
    vector space and detect changes if any according to a user-defined confidence
    level; an estimate for the change point is provided as well. We ground our methods
    on theoretical results that show how the inference in the numerical vector space
    is related to the one in graph domain, and vice-versa.
  bibtex: >
    @article{zambon2019changepoint,
      title = {Change-{{Point Methods}} on a {{Sequence}} of {{Graphs}}},
      author = {Zambon, Daniele and Alippi, Cesare and Livi, Lorenzo},
      year = {2019},
      month = dec,
      journal = {IEEE Transactions on Signal Processing},
      volume = {67},
      number = {24},
      pages = {6327--6341},
      issn = {1941-0476},
      doi = {10.1109/TSP.2019.2953596}
    }
- title: Change Detection in Graph Streams by Learning Graph Embeddings on Constant-Curvature
    Manifolds
  links:
    paper: https://arxiv.org/abs/1805.06299
    doi: https://doi.org/10.1109/TNNLS.2019.2927301
    code: https://github.com/danielegrattarola/cdt-ccm-aae
  venue: IEEE Transactions on Neural Networks and Learning Systems
  year: 2020
  authors:
  - id:dgrattarola
  - id:dzambon
  - id:llivi
  - id:calippi
  keywords:
    - nonstationary environments
    - change detection
  abstract: We focus on the problem of detecting changes in stationarity in a stream
    of attributed graphs. To this end, we introduce a novel change detection framework
    based on neural networks and Constant-curvature manifolds (CCMs), that takes into
    account the non-Euclidean nature of graphs. Our contribution in this work is twofold.
    First, via a novel approach based on adversarial learning, we compute graph embeddings
    by training an autoencoder to represent graphs on CCMs. Second, we introduce two
    novel change detection tests operating on CCMs.
  bibtex: >
    @article{grattarola2020change,
      title = {Change {{Detection}} in {{Graph Streams}} by {{Learning Graph Embeddings}} on {{Constant-Curvature Manifolds}}},
      author = {Grattarola, Daniele and Zambon, Daniele and Livi, Lorenzo and Alippi, Cesare},
      year = {2020},
      month = jun,
      journal = {IEEE Transactions on Neural Networks and Learning Systems},
      volume = {31},
      number = {6},
      pages = {1856--1869},
      issn = {2162-237X, 2162-2388},
      doi = {10.1109/TNNLS.2019.2927301},
    }
- title: A Characterization of the Edge of Criticality in Binary Echo State Networks
  links:
    paper: https://arxiv.org/abs/1810.01742
  venue: IEEE International Workshop on Machine Learning for Signal Processing
  year: 2018
  authors:
  - id:pverzelli
  - id:llivi
  - id:calippi
  keywords:
    - reservoir computing
  abstract: We propose binary echo state networks (ESNs), which are architecturally
    equivalent to standard ESNs but consider binary activation functions and binary
    recurrent weights. For these networks, we derive a closed-form expression for
    the edge of criticality (EoC) in the autonomous case and perform simulations in
    order to assess their behavior in the case of noisy neurons and in the presence
    of a signal. We propose a theoretical explanation for the fact that the variance
    of the input plays a major role in characterizing the EoC.
- title: Anomaly and Change Detection in Graph Streams through Constant-Curvature
    Manifold Embeddings
  links:
    paper: https://arxiv.org/abs/1805.01360
    doi: https://doi.org/10.1109/IJCNN.2018.8489762
  venue: IEEE International Joint Conference on Neural Networks
  year: 2018
  authors:
  - id:dzambon
  - id:llivi
  - id:calippi
  keywords:
    - nonstationary environments
    - anomaly detection
    - change detection
  abstract: We investigate how embedding graphs on constant-curvature manifolds (hyper-spherical
    and hyperbolic manifolds) impacts on the ability to detect changes in sequences
    of attributed graphs. The proposed methodology consists in embedding graphs into
    a geometric space and perform change detection there by means of conventional
    methods for numerical streams.
  bibtex: >
    @inproceedings{zambon2018anomaly,
      title = {Anomaly and {{Change Detection}} in {{Graph Streams}} through {{Constant-Curvature Manifold Embeddings}}},
      booktitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
      author = {Zambon, Daniele and Livi, Lorenzo and Alippi, Cesare},
      year = {2018},
      month = jul,
      pages = {1--7},
      issn = {2161-4407},
      doi = {10.1109/IJCNN.2018.8489762}
    }
- title: Concept Drift and Anomaly Detection in Graph Streams
  links:
    paper: https://arxiv.org/abs/1706.06941
    doi: https://doi.org/10.1109/TNNLS.2018.2804443}
  venue: IEEE Transactions on Neural Networks and Learning Systems
  year: 2018
  authors:
  - id:dzambon
  - id:calippi
  - id:llivi
  keywords:
    - nonstationary environments
    - anomaly detection
    - change detection
  abstract: We consider stochastic processes generating graphs and propose a methodology
    for detecting changes in stationarity of such processes. The methodology acts
    by embedding every graph of the stream into a vector domain, where a conventional
    multivariate change detection procedure can be easily applied. We ground the soundness
    of our proposal by proving several theoretical results.
  bibtex: >
    @article{zambon2018concept,
      title = {Concept {{Drift}} and {{Anomaly Detection}} in {{Graph Streams}}},
      author = {Zambon, Daniele and Alippi, Cesare and Livi, Lorenzo},
      year = {2018},
      month = nov,
      journal = {IEEE Transactions on Neural Networks and Learning Systems},
      volume = {29},
      number = {11},
      pages = {5592--5605},
      issn = {2162-2388},
      doi = {10.1109/TNNLS.2018.2804443}
    }
- title: Detecting Changes in Sequences of Attributed Graphs
  links:
    paper: https://ieeexplore.ieee.org/document/8285273
    doi: https://doi.org/10.1109/SSCI.2017.8285273
  venue: IEEE Symposium Series on Computational Intelligence
  year: 2017
  authors:
  - id:dzambon
  - id:llivi
  - id:calippi
  keywords:
    - nonstationary environments
    - change detection
  abstract: We consider a methodology for detecting changes in sequences of graphs.
    Changes are recognized by embedding each graph into a vector space, where conventional
    change detection procedures exist and can be easily applied. We introduce the
    methodology and focus on expanding experimental evaluations on controlled yet
    relevant examples involving geometric graphs and Markov chains.
  bibtex: >
    @inproceedings{zambon2017detecting,
      title = {Detecting Changes in Sequences of Attributed Graphs},
      booktitle = {2017 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
      author = {Zambon, Daniele and Livi, Lorenzo and Alippi, Cesare},
      year = {2017},
      month = nov,
      pages = {1--7},
      doi = {10.1109/SSCI.2017.8285273}
    }
